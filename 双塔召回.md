# 双塔召回

## 经典双塔模型

双塔模型是在召回和粗排领域都十分经典且优质的选项.

- 优势: 能够满足在线预估时低延时的需求.
- 缺陷: 不能实现 user 和 item 特征之间的交叉.

### DSSM

DSSM (Deep Structured Semantic Method),         主要被应用于 NLP 领域相关语义相似度任务. 

DSSM 模型的核心原理是: 将 query 和 doc 映射到同一个共同维度的语义向量空间. 通过最大化query和doc语义向量之 间的余弦相似度,         从而训练得到隐含语义模型.

![](https://s1.vika.cn/space/2022/06/22/3f340345a2ef4077adc4c410f97cc672)

DSSM 将用户和 item 分离,         建立独立的子网络塔式结构,         然后计算双塔输出的 embedding 向量的相似度. 也正因如此,         需要注意双塔的 embedding 向量的维度需要一致,         即最后一层全连接层隐藏单元个数相同.

![](https://s1.vika.cn/space/2022/06/23/7f2ede42f4f149689ba0af91e15217f7)

在召回模型中,         将这种检索行为视为多类分类问题,         类似于 YouTubeDNN 模型. 将物料库中所有的 item 视为一个类别,         因此损失函数需要计算每个类的概率值:

![](https://s1.vika.cn/space/2022/06/23/ac6d27127c29467f95842790451188e9)

其中 $s(x,        y)$ 表示两个向量的相似度,         $P(y|x;\theta)$ 表示预测类别的概率,         M 表示物料库所有的 item.

DSSM 为双塔的经典模型,         优势在于在海量数据进行召回的场景下,         能够做到较快的响应速度,         这是因为双塔模型的结构简单,         没有进行 user 和 item 的特征交叉. 然而因为这点,         双塔模型缺乏特征交叉,         导致精确度上有一定的牺牲. 而下文介绍到的模型,         就是针对这些双塔模型因为模型结构而自然带有的缺陷进行优化而被提出的.

### SENet 双塔模型

![](https://s1.vika.cn/space/2022/06/23/26450a0408ec4a01b8c07d6620aa512f)

SENet 的模型结构由三个部分构成: Squeeze,         Excitation,         Re-weight.

- Squeeze 阶段将特征映射到 embedding 层
- Excitation 通过放缩将 embedding 层的维度进行缩小再放大,         从而达到信息的重建
- Reweight 阶段将前一阶段重置的参数乘到模型参数中

将 SENet 应用到双塔模型中,         如图所示:

![](https://s1.vika.cn/space/2022/06/23/676906f9c4094926a14bb15c592b7ba2)

可以将两个部分的特征使用不同的模型进行处理,         从而实现特征的自身交叉. 并且通过这种方式我们最后能够得到多个 embedding,         这样在进行相似度计算的时候进一步提高了两边特征的交叉.

在这个过程中,         SENet双塔模型主要是从特征选择的角度,         提高了两侧特征交叉的有效性,         减少了噪音对有效信息的干扰,         进而提高了双塔模型的效果.

### 多目标双塔模型

![](https://s1.vika.cn/space/2022/06/23/d5e61ef77af74b85a4b2244b4aa04aee)

如上图所示,        在user侧和item侧分别通过多个通道(DNN结构)为每个任务得到一个user embedding和item embedding,        然后针对不同的目标分别计算user 和 item 的相似度,        并计算各个目标的损失,        最后的优化目标可以是多个任务损失之和,        或者使用多任务学习中的动态损失权重.

这种模型结构,       可以针对多目标进行联合建模,       通过多任务学习的结构,       一方面可以利用不同任务之间的信息共享,       为一些稀疏特征提供其他任务中的迁移信息,       另一方面可以在召回时,       直接使用一个模型得到多个目标预测,       解决了多个模型维护困难的问题. 也就是说,       在线上通过这一个模型就可以同时得到多个指标,       例如视频场景,       一个模型就可以直接得到点赞,        评论,       转发等目标的预测值,       进而通过这些值计算分数获得最终的Top-K召回结果.

## 双塔模型细节

### 归一化和温度系数

在[Google的双塔召回模型](https://dl.acm.org/doi/pdf/10.1145/3298689.3346996)中,      重点介绍了两个trick,      将user和item侧输出的embedding进行归一化以及对于內积值除以温度系数,      实验证明这两种方式可以取得十分好的效果. 那为什么这两种方法会使得模型的效果更好呢?

- 归一化: 对 user 和 item 的 embedding 层的输出,      进行 L2 归一化.
$$
u(x,      \theta) ←=\frac{u(x,     θ)}{∣∣u(x,     θ)∣∣_2}​​
$$

- 温度系数: 将归一化后计算完成的相似度,      除以一个温度系数 r
$$s(u,     v)=\frac{<u(x,     θ),     v(x,     θ)>}{r}$$

归一化的操作主要原因是因为向量点积距离是非度量空间,     不满足三角不等式,     而归一化的操作使得点击行为转化成了欧式距离. 而 ANN 一般是通过计算欧式距离来计算 loss,     这样就能够保证训练和检索的一致性.

### 模型的应用

在实际的工业应用场景下,     双塔模型被分为离线训练和在线服务两个环节.

- 离线训练: 集中在 item 的集合计算 embedding,     因为 item 相对更加稳定,     而 user 的特征会随着在线访问的变化不断变化.
- 在线服务: 接收实时的 user 特征的变化,     并且输入到 DNN 中,     得到 user 特征的 embedding 参数,     将其输入 ANN 进行检索.

### 负样本采样

对于召回模型而言,    其负样本并不能和排序模型一样只使用展现未点击样本,    因为召回模型在线上面临的数据分布是全部的item,    而不仅仅是展现未点击样本。因此在离线训练时,    需要让其保证和线上分布尽可能一致,    所以在负样本的选择样要尽可能的增加很多未被曝光的item. 

下面简单的介绍一些常见的采样方法

#### 全局随机采样

从全局候选item里面随机抽取一定数量item做为召回模型的负样本。这样的方式实现简单,   也可以让模型尽可能的和线上保持一致的分布,   尽可能的多的让模型对于全局item有区分的能力. 

例如YoutubeDNN算法

#### 全局随机采样 + 热门打压

针对 item 的热度进行打压,   即对一些很热门的 item,   在进行负采样时适当过采样,   这可以尽可能的让模型对于负样本有更加细粒度的区分. 从而让模型关注更多一些非热门的 item.

#### Hard Negative 增强样本

Hard Negative指的是选取一部分匹配度适中的item,  能够增加模型在训练时的难度,  提升模型能学习到item之间细粒度上的差异.

#### Batch内随机选择负采样

将batch内选择除了正样本之外的其它Item, 做为负样本, 其本质就是利用其他样本的正样本随机采样作为自己的负样本.