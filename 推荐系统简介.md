
### 推荐系统

不需要用户提供明确的需求, 可以通过分析用户画像和历史行为给用户的兴趣进行建模, 从而主动推荐能够满足他们兴趣和需求的信息.

### 搜索 vs 推荐
![搜索 vs 推荐](https://s1.vika.cn/space/2022/06/04/4aff29c839c34ff0b3c97e15e00409bc)

推荐系统相比于传统的搜索引擎模式, 提供了更多用户探索可能感兴趣话题的渠道.

### 推荐系统的意义

- 推荐系统存在的前提  
	- 信息过载  
	- ⽤户需求不明确  
- 推荐系统的⽬标  
	- ⾼效连接⽤户和物品，发现长尾商品  
	- 留住⽤户和内容⽣产者，实现商业⽬标

### 经典架构

![](https://s1.vika.cn/space/2022/06/04/b98ef0dc91374b4194434961c7813548)

![](https://s1.vika.cn/space/2022/06/04/e06a57f7bc444ab985a520db9affe298)

![](https://s1.vika.cn/space/2022/06/04/711026d1adaf4aa080bebbe58f9d5f5b)

参考链接: https://developers.google.com/machine-learning/recommendation

### 深度学习在推荐系统中的应用

- Deep Neural Networks for YouTube Recommendations  
	-  https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf

## 推荐系统评估

### 产品生态

![](https://s1.vika.cn/space/2022/06/04/41602f74f6144e708d5f413cee20a71f)

### 常用的评估指标

- 对推荐效果
	- 准确率
	- 满意度
	- 覆盖率
	- 多样性
	- 惊喜度
	- 信任度
- 对推荐系统模型
	- 实时性
	- 鲁棒性
	- 可拓展性
	- 用户留存
	- 商业目标

### 反馈: Explicit vs Implicit

![](https://s1.vika.cn/space/2022/06/04/e897d9bf7cf7457c9e621897824e699e)

### 准确性

- 评分预测

	- Root Mean Square Error
	$$RMSE = \sqrt{\frac{\Sigma_{u,i \in T} (r_{ui} - \hat{r_{ui}})^2}{|T|}}$$
	- Mean Average Error
	$$MAE = \frac{\Sigma_{u,i \in T} |r_{ui} - \hat{r_{ui}}|}{|T|}$$
	

- topN 推荐

	- Recall (TR/T)
	$$Recall = \frac{\Sigma_{u \in U} |R(u) \cap T(u)|}{\Sigma_{u \in U} |T(u)|}$$
	
	- Precision (TR/R)
	$$Recall = \frac{\Sigma_{u \in U} |R(u) \cap T(u)|}{\Sigma_{u \in U} |R(u)|}$$

- 工业界评价指标

![](https://s1.vika.cn/space/2022/06/04/62bbfc3f65c94c2ab7eb6fbcaffabd10)

### 覆盖度

![](https://s1.vika.cn/space/2022/06/04/5fd1ef1b343b4001b3e5675fa980fbc9)



### 多样性&新颖性&惊喜性  
- 多样性：推荐列表中两两物品的不相似性。（相似性如何度量？  
- 新颖性：未曾关注的类别、作者；推荐结果的平均流⾏度  
- 惊喜性：历史不相似（惊）但很满意（喜）  

往往需要牺牲准确性  

### 平衡准确性和多样性: Explore & Exploit

- Exploitation：选择现在可能最佳的方案  
- Exploration：选择现在不确定的一些方案, 但未来可能会有高收益的方案

### 评估方法

- 问卷调查
- 离线评估
	- 只能在⽤户看到过的候选集上做评估，且跟线上真实效果存在偏差
	- 能评估少数指标
	- 速度快，不损害⽤户体验
- 在线评估: A/B Test

#### A/B Test

![](https://s1.vika.cn/space/2022/06/04/31c9203fc5f14adc9d696e70b50ead24)

### 多重实验框架

- 保留单层实验框架易⽤，快速的优点的同时，增加可扩展性，灵活 性，健壮性。  
- 核⼼思路：将参数划分到N个⼦集，每个⼦集都关联⼀个实验层， 每个请求会被N个实验处理，同⼀个参数不能出现在多个层中.

![](https://s1.vika.cn/space/2022/06/04/b049a30727a5409f839a6c5650e67bf9)

## 冷启动

### 推荐系统的冷启动问题

- 用户冷启动: 新用户的个性化推荐
- 物品冷启动: 新物品推荐给用户 (协同过滤)
- 系统冷启动

推荐系统依赖历史数据, 否则无法预测用户偏好, 因此需要"启动".

### 用户冷启动

1. 收集用户特征
2. 制造粗粒度选项, 供用户填写
3. transfer learning
4. 新老用户策略的差异
	- 冷启动阶段的新用户更趋向热门排行榜推荐
	- Explore & Exploit 力度有区分
	- 使用单独的特征和模型
	- 保护用户体验 (避免广告, 冷启动探索)

### 物品冷启动

平衡 Explore 和 Exploit 的平衡.

#### MAB 问题 (Multi-Armed Bandits)

- 你进了一家赌场，假设面前有K台老虎机（arms）。  
- 老虎机本质上就是个运气游戏，我们假设每台老虎机Armi都有一定概率Pi吐出一块钱, 或者不吐钱（ 概率1 - Pi ）。  
- 假设你手上只有T枚代币（tokens），而每摇一次老虎机都需要花费一枚代币，也就是说你一共只能摇T次  
- 那么如何做才能使得期望回报（expected reward）最大呢

设计出平衡 explore 和 exploit 的方法, 使得在尽可能找到更好的老虎机的同时保证挖掘的深度.

#### 贪心法 

- 方法A: 随机地摇 N 次老虎机
- 方法B: 先探索 m 次老虎机, 然后在这中间取平均收益最大的老虎机, 在剩余次数中一直摇这台机器
- $\epsilon - greedy$ 方法
	- 设定一个参数 $\epsilon$
	- 每一次选择时, 都有 $\epsilon$ 的概率进行随机选择, 有 $1- \epsilon$ 的概率选择当前平均收益最大的.
- $\epsilon - greedy$ 方法的改进
	- 将 $\epsilon$ 的值动态缩小. e.g. $\epsilon = \frac{1}{log(m) + 0.001}$

#### Thompson Sampling

我们继续进一步思考, 使用 $\epsilon - greedy$ 方法 是随机性的进行探索, 那么如果我们能够根据之前进行的操作结果进行最优化的探索和挖掘, 就能够省去很多无用的探索, 进一步提升算法的效率.

在 TS 方法中, 我们把老虎机的收益定义为 0-1 分布, 并设定返回 1 的概率为 $\theta_k$ , 这样对于第 $k$ 个老虎机的收益期望值同样可以表达为 $r_k = \theta_k$ . 在这里, TS 方法尝试了将第 k 个老虎机的收益期望 $\theta_k$ 看作一个 beta 分布的随机变量. 然后通过多次进行尝试, 基于每一次前面得到的数据调整 beta 分布的参数 $\alpha_k , \beta_k$ , 最后就可以得到一个更准确的随机变量, 也能产生更加合理的推断. 最后我们再在所有老虎机中找到平均收益最大的老虎机, 进行 exploit.

![](https://s1.vika.cn/space/2022/06/05/0312ebfd7da9442296985d2159ae5b2b)

那么 Thompson Sampling 为什么选择了 beta 分布来进行收益 $\theta_k$ 的描述呢, 因为 beta 分布是在 (0,1) 区间的连续概率分布, 首先定义域和概率符合. 其次, beta 分布的两个参数越大, beta 分布就会越窄; 这意味着在我们训练的过程中, 能够通过不断迭代参数的值得到一个更精确的估计.

![](https://s1.vika.cn/space/2022/06/05/f2edcb2b024847a8a89b1a6169d90ae0)

![](https://s1.vika.cn/space/2022/06/05/1757193c59594f5189ffa74e8e3918eb)

Thompson Sampling 方法具有天然的随机性, 这样就不需要再每次手动进行 explore 和 exploit 的判断了, 因为所有机器都有可能采样到高的期望回报, 所以自然地会随机进行 explore, 而如果一个机器真的很优秀, 在经过多次的采样迭代之后, 它就会固定在一个更窄的范围内采样, 这样就更稳定地采样到高期望回报, 从而有更大的概率选择到它, 也就实现了 exploit.

P.S. 个人觉得 TS 方法的理解上需要能够明白, 后验概率 $P(\theta_k|A)$ 是 beta 分布这一点不是开始给出的, 而是我们假设先验概率 $P(\theta_k)$ 为 beta 分布, 并结合通过每次的实验, 得到的似然度 $P(A|\theta_k)$  (这里似然度视为伯努利分布) , 从而计算证明了后验概率也为伯努利分布 $B(\alpha_k + r_t, \beta_k + 1 - r_t)$ .