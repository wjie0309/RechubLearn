# 经典精排模型

## 精排简述

精排, 也叫排序模型 (ranking) , 是推荐系统中涵盖算法最多, 也是技术含量最高的一个领域. 它将召回粗排后的信息进行精度最高的排序, 并且直接面对用户输出结果.

目前精排模型主要都是深度学习模型. 下图为王喆老师的 *深度学习推荐系统* 一书中的精排层演化路线.

![](https://s1.vika.cn/space/2022/06/18/41897eb6d8d545e98bdf98eb8a6ffaf9)

具体来看分为 DNN , Wide&Deep 两大块. 

本次学习之后的内容将会先重点介绍精排模型中的 DeepFM 和 DIN 两个模型. 这两个模型从上图演化路线中也可以看出来, 分别来自 Wide&Deep, DNN. 非常具有代表性.

## DeepFM

### 模型结构

![](https://s1.vika.cn/space/2022/06/18/ecee6af1044c425aba63dbca7a65fec2)

可以看见, DeepFM 是由一个 Deep 的神经网络以及一个 FM 结构组成的.

#### FM 部分

FM (Factorization Machine) 是一种根据 LR 改良的特征交叉模型. 它通过增加了一个二阶交叉项, 来考虑所有的二阶特征组合.

FM 的公式如下:

$$
y_{FM} = \omega_0 + \sum_{i=1}^{N} \omega_i x_i + \sum_{i=1}^{N}\sum_{j=1}^{N} \omega_{ij} x_i x_j
$$

可以看见, 如果直接计算二阶交叉项, 那么复杂度将为 $O(N^2)$ , 在工业计算中需要消耗大额的时间. 而这个时候通过将特征矩阵 $\omega_{ij}$ 分解成两个长度为 k 的隐向量矩阵. (这里也可以理解成将每个 id 都做 embedding 映射为一个向量) 通过这种方式, 我们就可以把计算复杂度降低为 $O(kN)$ , 从而大大提高效率.

![](https://s1.vika.cn/space/2022/06/18/e62f0a21e2f746a3970fc85d30f8bf80)

- FM 的优缺点

	- **优点**

		1.  通过向量内积作为交叉特征的权重，可以在数据非常稀疏的情况下还能有效的训练处交叉特征的权重（因为不需要两个特征同时不为零）
		2.  可以通过公式上的优化，得到O(nk)的计算复杂度，k一般比较小，所以基本上和n是正相关的，计算效率非常高
		3.  尽管推荐场景下的总体特征空间非常大，但是FM的训练和预测只需要处理样本中的非零特征，这也提升了模型训练和线上预测的速度
		4.  由于模型的计算效率高，并且在稀疏场景下可以自动挖掘长尾低频物料。所以在召回、粗排和精排三个阶段都可以使用。应用在不同阶段时，样本构造、拟合目标及线上服务都有所不同（注意FM用于召回时对于user和item相似度的优化）
		5.  其他优点及工程经验参考石塔西的文章

	- **缺点**
	
	1.  只能显示的做特征的二阶交叉，对于更高阶的交叉无能为力。对于此类问题，后续就提出了各类显示、隐式交叉的模型，来充分挖掘特征之间的关系

在 DeepFM 中, 我们通过利用 FM 更高效地提取交叉特征, 从而取代了 Wide 模块提取二阶交叉特征的作用.

#### Deep 部分

![](https://s1.vika.cn/space/2022/06/18/95ef4d7d3a984ab1a230194ce8fddd25)

Deep Module 使用全连接的方式将 Dense Embeddings 输入到 Hidden Layer 中, 这里使用 Dense Embeddings 就是推荐业界常用的处理 DNN 中参数爆炸的方式.

Embedding 层将所有 id 类特征对应的 embedding 向量 concat 到一起输入到 DNN 中, 输出的形式可以表示为:

$$
z_1 = [v_1, v_2, ... , v_m]
$$

在此之后, 每一层之后的输出都作为下一层的输入.

$$
z^L = \sigma (W^{L-1}z^{L-1} + b^{L-1})
$$

最后通过 sigmoid 激活得到输出

$$
y_{DNN} = \sigma(W^L z^L + b^L)
$$

### 代码实现

```Python
class DeepFM(torch.nn.Module):
    def __init__(self, deep_features, fm_features, mlp_params):
        super(DeepFM, self).__init__()
        self.deep_features = deep_features
        self.fm_features = fm_features
        self.deep_dims = sum([fea.embed_dim for fea in deep_features])
        self.fm_dims = sum([fea.embed_dim for fea in fm_features])
        self.linear = LR(self.fm_dims)  # 1-odrder interaction
        self.fm = FM(reduce_sum=True)  # 2-odrder interaction
        self.embedding = EmbeddingLayer(deep_features + fm_features)
        self.mlp = MLP(self.deep_dims, **mlp_params)

    def forward(self, x):
        input_deep = self.embedding(x, self.deep_features, squeeze_dim=True)  
        #[batch_size, deep_dims]
        input_fm = self.embedding(x, self.fm_features, squeeze_dim=False)  
        #[batch_size, num_fields, embed_dim]

        y_linear = self.linear(input_fm.flatten(start_dim=1))
        y_fm = self.fm(input_fm)
        y_deep = self.mlp(input_deep)  #[batch_size, 1]
        y = y_linear + y_fm + y_deep
        return torch.sigmoid(y.squeeze(1))
```

### DeepFM 的意义

- 高效地学习特征组合, 解决了 DNN 参数量过大的局限性
- 在 Wide&Deep 的基础上, 解决了 Wide 不能自动将低阶和高阶组合特征组合的痛点

## DIN

### 产生背景

1. 工业界推荐系统的数据: 大量的用户历史行为数据.
2. 传统神经网络 (Embedding & MLP 模型) 无法考虑用户历史数据背后隐藏的用户的兴趣.
3. Attention 机制在其他 AI 领域的广泛使用

### 模型结构

DIN (Deep Interest Network) 模型的完整了解需要先了解两块知识.

#### DIN 模型的数据和特征表示

工业上的CTR预测数据集一般都是 `multi-group categorial form` 的形式, 就是类别型特征最为常见, 这种数据集一般长这样: 

![](https://s1.vika.cn/space/2022/06/19/c85f5250871e43b297272f0c32b9e91f)

其中用红框标注出来的是用户历史行为数据, 如 `visited_cate_ids` 就是用户的历史商品列表, 这一般会是一个多值特征, 而且不同用户此特征的长度不同, 这个时候我们一般使用 `multi-hot` 编码来处理数据.

用例子: `[weekday=Friday, gender=Female, visited_cate_ids={Bag,Book}, ad_cate_id=Book]` 来说明, 其数据就会被编码为下图所示:

![](https://s1.vika.cn/space/2022/06/19/311605bd49ca4f9ca718bfb00ced50b1)

此时的数据还没有做特征交叉或者交互组合, 这些工作将交给神经网络进行.

#### Baseline 模型

Baseline 即为前文提到的传统的 Embedding&MLP 模型. 它被分为三大模块, Embedding layer, Pooling&Concat layer, MLP.

![](https://s1.vika.cn/space/2022/06/19/3130852267ca4027b7518c2e0ce45393)

- Embedding layer

	通过 Embedding 将高维稀疏特征转化为低维稠密向量. 每个离散特征下面都会对应着一个embedding词典, 维度是 $D\times K$, 这里的 D 表示的是隐向量的维度, 而 K 表示的是当前离散特征的唯一取值个数.

- Pooling&Concat layer

	在经过了 Embedding 层后, 我们得到的隐向量维度是不同的, 而后面 MLP 的全连接层是需要所有参数的长度一致的, 因此我们利用这个层来把输出的隐向量转化为固定的长度. 同时 Pooling 层也起到特征降维的作用, 防止过拟合.
	
$$ei​=pooling(ei1​,ei2​,...eik​)$$

其中 k 为历史特种组里面用户购买过的商品数量. 在特征长度调整完成后, 用 concat 将他们拼接, 作为 MLP 的输入.

- MLP

	普通的全连接层, 来学习特征的交互.

- Loss

	Loss 使用的是对数似然:
	
	$$L=−\frac{1}{N}​\sum_{(x,y)∈S}​(ylogp(x)+(1−y)log(1−p(x)))$$
	
	这是二分类任务常用的损失函数.

而当下 Baseline 模型的问题是, 在进入 MLP 交互之前, 特征之间没有任何关联, 从而丢失了历史商品行为中各个商品对当前预测的重要程度. 为了解决这个问题, 我们在**当前候选广告和用户的历史行为之间引入注意力的机制**, 从而使得在预测当前商品的时候, 那些过去与当前商品更加相关的商品更能促进用户的点击行为.

#### DIN 模型架构

在 Baseline 的基础上, DIN 添加了一个注意力机制来学习用户兴趣和当前广告的关联程度. **能够根据用户历史行为特征和当前广告的相关性给用户历史行为特征embedding进行加权**.

![](https://s1.vika.cn/space/2022/06/19/c2e3c63ea39d4ee9bb0c604800308216)

相比 Base model, 这里添加了一个 Activation unit, 这里是一个前馈神经网络, 输入是用户历史行为商品和当前的候选商品, 输出为当前商品和历史商品的相关性. 输出的相关性和用户历史行为数据的 Embedding 相乘就能得到该用户的兴趣表示:

$$
v_U(A) = \sum_{j=1}^{H}a(e_j, v_A)e_j = \sum_{j=1}^{H}\omega_j e_j
$$

其中 $\omega_j$ 代表了第 j 个历史商品和当前候选商品 A 的相关度. 除此之外, 如模型中所示, 输入除了历史行为向量和候选广告向量外, 还加了一个它俩的外积操作, 作者说这里是有利于模型相关性建模的显性知识.

### DIN 实现

![](https://s1.vika.cn/space/2022/06/19/3aaa195a77324d8ab628e5c2889fc968)

```python
# DIN网络搭建
def DIN(feature_columns, behavior_feature_list, behavior_seq_feature_list):
    """
    这里搭建DIN网络，有了上面的各个模块，这里直接拼起来
    :param feature_columns: A list. 里面的每个元素是namedtuple(元组的一种扩展类型，同时支持序号和属性名访问组件)类型，表示的是数据的特征封装版
    :param behavior_feature_list: A list. 用户的候选行为列表
    :param behavior_seq_feature_list: A list. 用户的历史行为列表
    """
    # 构建Input层并将Input层转成列表作为模型的输入
    input_layer_dict = build_input_layers(feature_columns)
    input_layers = list(input_layer_dict.values())
    
    # 筛选出特征中的sparse和Dense特征， 后面要单独处理
    sparse_feature_columns = list(filter(lambda x: isinstance(x, SparseFeat), feature_columns))
    dense_feature_columns = list(filter(lambda x: isinstance(x, DenseFeat), feature_columns))
    
    # 获取Dense Input
    dnn_dense_input = []
    for fc in dense_feature_columns:
        dnn_dense_input.append(input_layer_dict[fc.name])
    
    # 将所有的dense特征拼接
    dnn_dense_input = concat_input_list(dnn_dense_input)   # (None, dense_fea_nums)
    
    # 构建embedding字典
    embedding_layer_dict = build_embedding_layers(feature_columns, input_layer_dict)

    # 离散的这些特特征embedding之后，然后拼接，然后直接作为全连接层Dense的输入，所以需要进行Flatten
    dnn_sparse_embed_input = concat_embedding_list(sparse_feature_columns, input_layer_dict, embedding_layer_dict, flatten=True)
    
    # 将所有的sparse特征embedding特征拼接
    dnn_sparse_input = concat_input_list(dnn_sparse_embed_input)   # (None, sparse_fea_nums*embed_dim)
    
    # 获取当前行为特征的embedding， 这里有可能有多个行为产生了行为列表，所以需要列表将其放在一起
    query_embed_list = embedding_lookup(behavior_feature_list, input_layer_dict, embedding_layer_dict)
    
    # 获取历史行为的embedding， 这里有可能有多个行为产生了行为列表，所以需要列表将其放在一起
    keys_embed_list = embedding_lookup(behavior_seq_feature_list, input_layer_dict, embedding_layer_dict)
    # 使用注意力机制将历史行为的序列池化，得到用户的兴趣
    dnn_seq_input_list = []
    for i in range(len(keys_embed_list)):
        seq_embed = AttentionPoolingLayer()([query_embed_list[i], keys_embed_list[i]])  # (None, embed_dim)
        dnn_seq_input_list.append(seq_embed)
    
    # 将多个行为序列的embedding进行拼接
    dnn_seq_input = concat_input_list(dnn_seq_input_list)  # (None, hist_len*embed_dim)
    
    # 将dense特征，sparse特征， 即通过注意力机制加权的序列特征拼接起来
    dnn_input = Concatenate(axis=1)([dnn_dense_input, dnn_sparse_input, dnn_seq_input]) # (None, dense_fea_num+sparse_fea_nums*embed_dim+hist_len*embed_dim)
    
    # 获取最终的DNN的预测值
    dnn_logits = get_dnn_logits(dnn_input, activation='prelu')
    
    model = Model(inputs=input_layers, outputs=dnn_logits)
    
    return model
```